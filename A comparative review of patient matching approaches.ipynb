{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A comparative review of patient matching approaches (Deduplication)\n",
    "**This notebook compares different patient mactching approaches while demonstrating those concepts using python** </br>\n",
    "**05 May 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mayer Antoine**<br /> \n",
    "Public Health Informatics Fellow <br /> \n",
    "DGHT/Health Informatics Team <br /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the past years HIV program in PEPFAR supported countries have committed significant effort and resources to achieve the 95-95-95 target to end the AIDS by 2030. However, to successfully monitor their progress on the fast track strategy they need national patient-level longitudinal data repository of people living with HIV, tracking their HIV service and care from testing to death. To create such complete longitudinal patient record, countries need to link and deduplicate, on a regular basis, record from multiple data sources such as testing registry, art patient monitoring system, pharmacy-dispensing system, laboratory information system and case-based surveillance system.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patient matching strategies has been categorized over the years in 3 main categories: **deterministic, probabilistic and modern or machine learning**.\n",
    "Choosing and developing the best approach to use in a given situation to match or deduplicate HIV patient records depends on the context, data quality, skills and resources available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several algorithm and tools has been presented and implemented in multiple studies, also previous research has compared deterministic and probabilistic matching  but a few offer a concise comparison of the different approaches and provides practical guidance of selecting  an algorithm and operationalizing the model in a production environment either  for patient care or for HIV public health surveillance.  Our aim in this paper is to compare traditional and modern approach of patient matching techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conducted an experiment comparing the different approaches by looking the effectiveness of some the prominent algorithm available in each approach. As shown in table 2, we selected 2 algorithms for each approach. Each algorithm was used to deduplicate a synthetic datasets and link two synthetic datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Matching Approach| Algorithm |\n",
    "| --- | --- | \n",
    "| Deterministic | Exact matching of patient demographics | \n",
    "|   | Composite Key OR pseudo identifier| \n",
    "| Probabilistic | Threshold-based Similarity Sum (SimSum) | \n",
    "|  |Threshold-based Weighted average|\n",
    "|  | Expectation Maximization (EM) |\n",
    "| Machine learning | Naive Bayes |\n",
    "|  | Support Vector Machine (SVM) | \n",
    "|  |Logistic Regression |\n",
    "\n",
    "\n",
    "\n",
    "The python record linkage toolkit (PRLT) was used during all steps of the experiment. Itâ€™s a library to link records in or between data sources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient matching process (deduplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![record linkage process](./docs/matching_process_dedup.PNG \"record linkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import patientlinkr as linkr\n",
    "from metaphone import doublemetaphone\n",
    "\n",
    "import recordlinkage as rl\n",
    "from recordlinkage.preprocessing import phonetic,clean\n",
    "from recordlinkage.datasets import load_febrl2,load_febrl1,load_febrl3,load_febrl4\n",
    "from recordlinkage.datasets.febrl import _febrl_links\n",
    "from recordlinkage.index import Block,SortedNeighbourhood\n",
    "from recordlinkage.classifiers import KMeansClassifier , ECMClassifier ,NaiveBayesClassifier,SVMClassifier,LogisticRegressionClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 0.25.0 \n",
      "\n",
      "Python Record Linkage version: 0.14 \n",
      "\n",
      "Numpy version: 1.16.3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get Version information\n",
    "print(\"Pandas version: {0}\".format(pd.__version__),'\\n')\n",
    "print(\"Python Record Linkage version: {0}\".format(rl._version.get_versions()['version']),'\\n')\n",
    "print(\"Numpy version: {0}\".format(np.__version__),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So all output comes through from Ipython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importing dataset to deduplicate\n",
    "Before we start with the deduplication process first we import the data in notebook. We assume the data has been cleaned and mapped to be imported in the format below :\n",
    "\n",
    "| Variable| Description | Format |\n",
    "| --- | --- | --- |\n",
    "| rec_id | Internal ID in your data | |\n",
    "| given_name | patient first name | |\n",
    "| surname | patient last name | |\n",
    "| date_of_birth | patient date of birth | YYYYMMDD  |\n",
    "| sex | patient sex | |\n",
    "\n",
    "In this notebook we use  synthetic dataset from The Python Record Linkage Toolkit (PRLT). The PRLT contains several open public synthetic datasets. The package is distributed with a  four synthetic datasets. For this project we will use The Freely Extensible Biomedical Record Linkage (Febrl) dataset 3 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def run_import_data(file_name):\n",
    "    # file to deduplicate\n",
    "    IMPORT_FILE_TO_DEDUPLICATE = \"./datasets/\"+file_name\n",
    "    print(IMPORT_FILE_TO_DEDUPLICATE)\n",
    "\n",
    "    df_a = pd.read_csv(IMPORT_FILE_TO_DEDUPLICATE,\n",
    "                        index_col=\"rec_id\",\n",
    "                        sep=\",\",\n",
    "                        engine='c',\n",
    "                        skipinitialspace=True,\n",
    "                        encoding='utf-8',\n",
    "                        parse_dates=[\"date_of_birth\"])\n",
    "    #df_a = df_a.drop(['culture','blocking_number'],axis =1)\n",
    "    df_true_links = _febrl_links(df_a).to_frame(index=False)\n",
    "    df_true_links.columns=['rec_id_1','rec_id_2']\n",
    "    df_true_links.set_index(['rec_id_1','rec_id_2'],inplace=True)\n",
    "    df_a = df_a[['given_name','surname','sex','date_of_birth']]\n",
    "    df_a.sort_index().head()\n",
    "    \n",
    "    return df_a,df_true_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_preprocessing(df_a):\n",
    "    df_a['given_name'] = clean(df_a['given_name'])\n",
    "    df_a['surname'] = clean(df_a['surname'])\n",
    "    #df_a['date_of_birth'] = clean(df_a['date_of_birth'])\n",
    "\n",
    "    df_a['date_of_birth'] = pd.to_datetime(df_a['date_of_birth'],format='%Y%m%d', errors='coerce')\n",
    "    df_a['YearB'] = df_a['date_of_birth'].dt.year.astype('Int64') # df_a['date_of_birth'].str[:4].astype(str)\n",
    "    df_a['MonthB'] = df_a['date_of_birth'].dt.month.astype('Int64')  # df_a['date_of_birth'].str[5:7].astype(str)\n",
    "    df_a['DayB'] = df_a['date_of_birth'].dt.day.astype('Int64')  # df_a['date_of_birth'].str[6:].astype(str)\n",
    "\n",
    "    #df_a['metaphone_given_name'] = phonetic(df_a['given_name'], method='metaphone')\n",
    "    #df_a['metaphone_surname'] = phonetic(df_a['surname'], method='metaphone')\n",
    "\n",
    "    df_a['dbmetaphone_given_name'] =df_a['given_name'].apply(lambda x: doublemetaphone(str(x))[0] if(np.all(pd.notnull(x))) else x)\n",
    "    df_a['dbmetaphone_surname'] = df_a['surname'].apply(lambda x: doublemetaphone(str(x))[0] if(np.all(pd.notnull(x))) else x)\n",
    "    df_a['pseudo_unique_id'] = linkr._create_pseudo_id(df_a)\n",
    "    \n",
    "    return df_a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deduplication function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_classification(df_a,df_true_links):\n",
    "    \n",
    "    df_a = df_a.copy()\n",
    "    df_true_links = df_true_links.copy()\n",
    "\n",
    "    blocks =[[\"dbmetaphone_given_name\",\"dbmetaphone_surname\"],\n",
    "                  [\"dbmetaphone_given_name\", \"date_of_birth\"],\n",
    "                  [\"dbmetaphone_surname\",\"sex\"],\n",
    "                  [\"dbmetaphone_surname\", \"date_of_birth\"],\n",
    "                ['date_of_birth']]\n",
    "\n",
    "    ## exact matching rules\n",
    "    b = [['given_name','surname','date_of_birth',],\n",
    "            ['given_name','surname']]\n",
    "\n",
    "    ## pseudo_unique identifier\n",
    "    bunique = [['pseudo_unique_id']]\n",
    "\n",
    "\n",
    "    # #string comparison configuration\n",
    "    comparison = [{ \"vartype\":\"string\", \"field\": \"given_name\",\"method\":\"jarowinkler\", \"threshold\":0.85, \"code\":\"given_name\"},\n",
    "                             { \"vartype\":\"string\", \"field\": \"surname\",\"method\":\"jarowinkler\", \"threshold\":0.85, \"code\":\"surname\"},\n",
    "                             { \"vartype\":\"exact\", \"field\": \"date_of_birth\", \"code\":\"date_of_birth\"},\n",
    "                             { \"vartype\":\"exact\", \"field\": \"sex\",\"code\":\"sex\"}\n",
    "\n",
    "                     ]\n",
    "\n",
    "    comparison_complex = [{ \"vartype\":\"string\", \"field\": \"given_name\",\"method\":\"jarowinkler\", \"code\":\"given_name\"},\n",
    "                             { \"vartype\":\"string\", \"field\": \"surname\",\"method\":\"jarowinkler\", \"code\":\"surname\"},\n",
    "                             { \"vartype\":\"exact\", \"field\": \"date_of_birth\", \"code\":\"date_of_birth\"},\n",
    "                             { \"vartype\":\"exact\", \"field\": \"sex\",\"code\":\"sex\"},\n",
    "                     ]\n",
    "\n",
    "    weigth_factor = {'given_name':2,\n",
    "                     'surname':2, \n",
    "                     'date_of_birth':1,\n",
    "                     'sex':1}\n",
    "\n",
    "\n",
    "    approach = [\"Deterministic\",\"Probabilistic\",\"Machine learning\"]\n",
    "\n",
    "    # list of classifiers to compare\n",
    "    classifiers = [(\"Exact Matching\", linkr.ExactMatchingClassifier()),\n",
    "                   (\"Pseudo Unique ID\", linkr.ExactMatchingClassifier()),\n",
    "                   (\"ECM\", ECMClassifier()),\n",
    "                   (\"SimSum\",linkr.SimSumClassifier(threshold = 3)),\n",
    "                   (\"WeightedAverage\",linkr.WeightedAverageClassifier(0.75,weigth_factor)),\n",
    "                   (\"NaivesBayes\",NaiveBayesClassifier()),\n",
    "                   (\"SVM\",SVMClassifier()),\n",
    "                   (\"LogisticRegression\",LogisticRegressionClassifier()),\n",
    "                  ]\n",
    "    metrics = {}\n",
    "\n",
    "    print('Number of records:',len(df_a))\n",
    "    # iterate over classifiers\n",
    "    for name, classifier in classifiers:\n",
    "\n",
    "        # determnistic\n",
    "        if(name == 'Exact Matching'):\n",
    "            indexer = linkr.BlockUnion(block_on = b)\n",
    "            pairs = indexer.index(df_a)\n",
    "            match = classifier.fit_predict(pairs)\n",
    "            nunique  = linkr.get_unique(df_a,match)\n",
    "            matrix,precision,recall,fscore = linkr.metrics(df_true_links,match,pairs.to_frame(index=True))\n",
    "            TP,FP,TN,FN = matrix[0,0],matrix[1,0],matrix[1,1],matrix[0,1]\n",
    "            match_len = len(match)\n",
    "\n",
    "\n",
    "        elif(name == 'Pseudo Unique ID'):\n",
    "            indexer = linkr.BlockUnion(block_on = bunique) \n",
    "            pairs = indexer.index(df_a)\n",
    "            match = classifier.fit_predict(pairs)\n",
    "            nunique  = linkr.get_unique(df_a,match)\n",
    "            matrix,precision,recall,fscore = linkr.metrics(df_true_links,match,pairs.to_frame(index=True))\n",
    "            TP,FP,TN,FN = matrix[0,0],matrix[1,0],matrix[1,1],matrix[0,1]\n",
    "            match_len = len(match)\n",
    "\n",
    "\n",
    "        # probabilistic\n",
    "        elif(name in ['ECM']):\n",
    "            indexer = linkr.BlockUnion(block_on = blocks)\n",
    "            pairs = indexer.index(df_a)  \n",
    "            compare_cl = linkr.Comparator(compare_on = comparison)\n",
    "            comparison_vectors = compare_cl.compute_compare(pairs,df_a)\n",
    "            match = classifier.fit_predict(comparison_vectors)\n",
    "            nunique  = linkr.get_unique(df_a,match)\n",
    "            matrix,precision,recall,fscore = linkr.metrics(df_true_links,match,comparison_vectors)\n",
    "            TP,FP,TN,FN = matrix[0,0],matrix[1,0],matrix[1,1],matrix[0,1]\n",
    "            match_len = len(match)\n",
    "\n",
    "        elif(name in ['SimSum','WeightedAverage']):\n",
    "            indexer = linkr.BlockUnion(block_on = blocks)\n",
    "            pairs = indexer.index(df_a)  \n",
    "            # note complex comparison\n",
    "            compare_cl = linkr.Comparator(compare_on = comparison_complex)\n",
    "            comparison_vectors = compare_cl.compute_compare(pairs,df_a)\n",
    "            match = classifier.fit_predict(comparison_vectors)\n",
    "            nunique  = linkr.get_unique(df_a,match)\n",
    "            matrix,precision,recall,fscore = linkr.metrics(df_true_links,match,comparison_vectors)\n",
    "            TP,FP,TN,FN = matrix[0,0],matrix[1,0],matrix[1,1],matrix[0,1]\n",
    "            match_len = len(match)\n",
    "\n",
    "        # machine learning \n",
    "        elif(name in ['NaivesBayes','SVM','LogisticRegression']):\n",
    "            indexer = linkr.BlockUnion(block_on = blocks)\n",
    "            pairs = indexer.index(df_a)  \n",
    "            compare_cl = linkr.Comparator(compare_on = comparison)\n",
    "            comparison_vectors = compare_cl.compute_compare(pairs,df_a)      \n",
    "            #note cross validation\n",
    "            match = linkr.cross_val_predict(classifier,comparison_vectors,df_true_links,cv=10,method = 'predict')\n",
    "            nunique  = linkr.get_unique(df_a,match)\n",
    "            matrix,precision,recall,fscore = linkr.metrics(df_true_links,match,comparison_vectors)\n",
    "            TP,FP,TN,FN = matrix[0,0],matrix[1,0],matrix[1,1],matrix[0,1]\n",
    "            match_len = len(match)\n",
    "\n",
    "\n",
    "        precision = round(precision*100,2)\n",
    "        recall = round(recall*100,2)\n",
    "        fscore = round(fscore*100,2)\n",
    "        metrics[name] = [len(pairs),match_len,TP,FP,FN,precision,recall,fscore,nunique]\n",
    "        print(\"Score for %s: %0.1f%% \" % (name, fscore))\n",
    "        \n",
    "    return metrics\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset A\n",
    "* **Dataset-A contains 5000 records (2500 originals and 25000 duplicates)**.\n",
    "* **one duplicate maximum per records**\n",
    "* **one modification maximum per field**\n",
    "* **one modification maximum per record** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/dataset_a.csv\n",
      "Number of records: 5000\n",
      "Score for Exact Matching: 31.1% \n",
      "Score for Pseudo Unique ID: 80.4% \n",
      "Score for ECM: 95.9% \n",
      "Score for SimSum: 80.0% \n",
      "Score for WeightedAverage: 93.4% \n",
      "Score for NaivesBayes: 97.5% \n",
      "Score for SVM: 97.9% \n",
      "Score for LogisticRegression: 97.9% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84.27499999999999"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairs</th>\n",
       "      <th>matched</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>presicion</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>6665.0</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>95.96</td>\n",
       "      <td>97.94</td>\n",
       "      <td>2601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>6665.0</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>95.96</td>\n",
       "      <td>97.94</td>\n",
       "      <td>2601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaivesBayes</th>\n",
       "      <td>6665.0</td>\n",
       "      <td>2392.0</td>\n",
       "      <td>2385.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>99.71</td>\n",
       "      <td>95.40</td>\n",
       "      <td>97.51</td>\n",
       "      <td>2612.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECM</th>\n",
       "      <td>6665.0</td>\n",
       "      <td>2316.0</td>\n",
       "      <td>2309.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>99.70</td>\n",
       "      <td>92.36</td>\n",
       "      <td>95.89</td>\n",
       "      <td>2688.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeightedAverage</th>\n",
       "      <td>6665.0</td>\n",
       "      <td>2191.0</td>\n",
       "      <td>2190.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>99.95</td>\n",
       "      <td>87.60</td>\n",
       "      <td>93.37</td>\n",
       "      <td>2809.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pseudo Unique ID</th>\n",
       "      <td>1768.0</td>\n",
       "      <td>1768.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>785.0</td>\n",
       "      <td>97.00</td>\n",
       "      <td>68.60</td>\n",
       "      <td>80.37</td>\n",
       "      <td>3255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimSum</th>\n",
       "      <td>6665.0</td>\n",
       "      <td>1728.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>808.0</td>\n",
       "      <td>97.92</td>\n",
       "      <td>67.68</td>\n",
       "      <td>80.04</td>\n",
       "      <td>3291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exact Matching</th>\n",
       "      <td>461.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2039.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>18.44</td>\n",
       "      <td>31.14</td>\n",
       "      <td>4539.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pairs  matched      TP    FP      FN  presicion  recall  \\\n",
       "SVM                 6665.0   2399.0  2399.0   0.0   101.0     100.00   95.96   \n",
       "LogisticRegression  6665.0   2399.0  2399.0   0.0   101.0     100.00   95.96   \n",
       "NaivesBayes         6665.0   2392.0  2385.0   7.0   115.0      99.71   95.40   \n",
       "ECM                 6665.0   2316.0  2309.0   7.0   191.0      99.70   92.36   \n",
       "WeightedAverage     6665.0   2191.0  2190.0   1.0   310.0      99.95   87.60   \n",
       "Pseudo Unique ID    1768.0   1768.0  1715.0  53.0   785.0      97.00   68.60   \n",
       "SimSum              6665.0   1728.0  1692.0  36.0   808.0      97.92   67.68   \n",
       "Exact Matching       461.0    461.0   461.0   0.0  2039.0     100.00   18.44   \n",
       "\n",
       "                    fscore  nunique  \n",
       "SVM                  97.94   2601.0  \n",
       "LogisticRegression   97.94   2601.0  \n",
       "NaivesBayes          97.51   2612.0  \n",
       "ECM                  95.89   2688.0  \n",
       "WeightedAverage      93.37   2809.0  \n",
       "Pseudo Unique ID     80.37   3255.0  \n",
       "SimSum               80.04   3291.0  \n",
       "Exact Matching       31.14   4539.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a,df_true_links = run_import_data('dataset_a.csv')\n",
    "df_a = run_preprocessing(df_a)\n",
    "metrics = run_classification(df_a,df_true_links)\n",
    "df_results = pd.DataFrame(metrics,index= ['pairs','matched','TP','FP','FN','presicion','recall','fscore','nunique'])\n",
    "df_results_t = df_results.T.sort_values('fscore',ascending=False)\n",
    "df_results_t.to_csv(\"dataset_a_results.csv\")\n",
    "df_results_t['fscore'].mean()\n",
    "df_results_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset B\n",
    "* **Dataset-B contains 5000 records (4000 originals and 1000 duplicates)**.\n",
    "* **5 duplicates maximum per records**\n",
    "* **2 modifications maximum per field**\n",
    "* **2 modifications maximum per record** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/dataset_b.csv\n",
      "Score for Exact Matching: 14.8% \n",
      "Score for Pseudo Unique ID: 64.7% \n",
      "Score for ECM: 87.8% \n",
      "Score for SimSum: 67.1% \n",
      "Score for WeightedAverage: 87.8% \n",
      "Score for NaivesBayes: 88.5% \n",
      "Score for SVM: 92.5% \n",
      "Score for LogisticRegression: 92.5% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74.4575"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairs</th>\n",
       "      <th>matched</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>presicion</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>5278.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>99.88</td>\n",
       "      <td>86.1</td>\n",
       "      <td>92.48</td>\n",
       "      <td>4138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>5278.0</td>\n",
       "      <td>862.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>99.88</td>\n",
       "      <td>86.1</td>\n",
       "      <td>92.48</td>\n",
       "      <td>4138.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaivesBayes</th>\n",
       "      <td>5278.0</td>\n",
       "      <td>803.0</td>\n",
       "      <td>798.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>99.38</td>\n",
       "      <td>79.8</td>\n",
       "      <td>88.52</td>\n",
       "      <td>4197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECM</th>\n",
       "      <td>5278.0</td>\n",
       "      <td>792.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>99.37</td>\n",
       "      <td>78.7</td>\n",
       "      <td>87.83</td>\n",
       "      <td>4208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeightedAverage</th>\n",
       "      <td>5278.0</td>\n",
       "      <td>795.0</td>\n",
       "      <td>788.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>99.12</td>\n",
       "      <td>78.8</td>\n",
       "      <td>87.80</td>\n",
       "      <td>4205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimSum</th>\n",
       "      <td>5278.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>532.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>90.78</td>\n",
       "      <td>53.2</td>\n",
       "      <td>67.09</td>\n",
       "      <td>4429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pseudo Unique ID</th>\n",
       "      <td>550.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>501.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>91.09</td>\n",
       "      <td>50.1</td>\n",
       "      <td>64.65</td>\n",
       "      <td>4461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exact Matching</th>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.81</td>\n",
       "      <td>4920.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pairs  matched     TP    FP     FN  presicion  recall  \\\n",
       "SVM                 5278.0    862.0  861.0   1.0  139.0      99.88    86.1   \n",
       "LogisticRegression  5278.0    862.0  861.0   1.0  139.0      99.88    86.1   \n",
       "NaivesBayes         5278.0    803.0  798.0   5.0  202.0      99.38    79.8   \n",
       "ECM                 5278.0    792.0  787.0   5.0  213.0      99.37    78.7   \n",
       "WeightedAverage     5278.0    795.0  788.0   7.0  212.0      99.12    78.8   \n",
       "SimSum              5278.0    586.0  532.0  54.0  468.0      90.78    53.2   \n",
       "Pseudo Unique ID     550.0    550.0  501.0  49.0  499.0      91.09    50.1   \n",
       "Exact Matching        80.0     80.0   80.0   0.0  920.0     100.00     8.0   \n",
       "\n",
       "                    fscore  nunique  \n",
       "SVM                  92.48   4138.0  \n",
       "LogisticRegression   92.48   4138.0  \n",
       "NaivesBayes          88.52   4197.0  \n",
       "ECM                  87.83   4208.0  \n",
       "WeightedAverage      87.80   4205.0  \n",
       "SimSum               67.09   4429.0  \n",
       "Pseudo Unique ID     64.65   4461.0  \n",
       "Exact Matching       14.81   4920.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a,df_true_links = run_import_data('dataset_b.csv')\n",
    "df_a = run_preprocessing(df_a)\n",
    "metrics = run_classification(df_a,df_true_links)\n",
    "df_results = pd.DataFrame(metrics,index= ['pairs','matched','TP','FP','FN','presicion','recall','fscore','nunique'])\n",
    "df_results_t = df_results.T.sort_values('fscore',ascending=False)\n",
    "df_results_t.to_csv(\"dataset_b_results.csv\")\n",
    "df_results_t['fscore'].mean()\n",
    "df_results_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset C\n",
    "* **Dataset-C contains 5000 records (3000 originals and 2000 duplicates)**.\n",
    "* **5 duplicates maximum per records**\n",
    "* **2 modifications maximum per field**\n",
    "* **2 modifications maximum per record** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/dataset_c.csv\n",
      "Score for Exact Matching: 16.3% \n",
      "Score for Pseudo Unique ID: 62.5% \n",
      "Score for ECM: 92.0% \n",
      "Score for SimSum: 67.7% \n",
      "Score for WeightedAverage: 87.9% \n",
      "Score for NaivesBayes: 92.0% \n",
      "Score for SVM: 92.0% \n",
      "Score for LogisticRegression: 92.0% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.3025"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pairs</th>\n",
       "      <th>matched</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>presicion</th>\n",
       "      <th>recall</th>\n",
       "      <th>fscore</th>\n",
       "      <th>nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>5965.0</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>1707.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>99.88</td>\n",
       "      <td>85.35</td>\n",
       "      <td>92.05</td>\n",
       "      <td>3292.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECM</th>\n",
       "      <td>5965.0</td>\n",
       "      <td>1714.0</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>99.71</td>\n",
       "      <td>85.45</td>\n",
       "      <td>92.03</td>\n",
       "      <td>3287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaivesBayes</th>\n",
       "      <td>5965.0</td>\n",
       "      <td>1714.0</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>99.71</td>\n",
       "      <td>85.45</td>\n",
       "      <td>92.03</td>\n",
       "      <td>3287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>5965.0</td>\n",
       "      <td>1713.0</td>\n",
       "      <td>1708.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>99.71</td>\n",
       "      <td>85.40</td>\n",
       "      <td>92.00</td>\n",
       "      <td>3288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeightedAverage</th>\n",
       "      <td>5965.0</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>1571.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>99.75</td>\n",
       "      <td>78.55</td>\n",
       "      <td>87.89</td>\n",
       "      <td>3427.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SimSum</th>\n",
       "      <td>5965.0</td>\n",
       "      <td>1068.0</td>\n",
       "      <td>1038.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>962.0</td>\n",
       "      <td>97.19</td>\n",
       "      <td>51.90</td>\n",
       "      <td>67.67</td>\n",
       "      <td>3939.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pseudo Unique ID</th>\n",
       "      <td>1015.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>942.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1058.0</td>\n",
       "      <td>92.81</td>\n",
       "      <td>47.10</td>\n",
       "      <td>62.49</td>\n",
       "      <td>4014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exact Matching</th>\n",
       "      <td>177.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1823.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>8.85</td>\n",
       "      <td>16.26</td>\n",
       "      <td>4823.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pairs  matched      TP    FP      FN  presicion  recall  \\\n",
       "SVM                 5965.0   1709.0  1707.0   2.0   293.0      99.88   85.35   \n",
       "ECM                 5965.0   1714.0  1709.0   5.0   291.0      99.71   85.45   \n",
       "NaivesBayes         5965.0   1714.0  1709.0   5.0   291.0      99.71   85.45   \n",
       "LogisticRegression  5965.0   1713.0  1708.0   5.0   292.0      99.71   85.40   \n",
       "WeightedAverage     5965.0   1575.0  1571.0   4.0   429.0      99.75   78.55   \n",
       "SimSum              5965.0   1068.0  1038.0  30.0   962.0      97.19   51.90   \n",
       "Pseudo Unique ID    1015.0   1015.0   942.0  73.0  1058.0      92.81   47.10   \n",
       "Exact Matching       177.0    177.0   177.0   0.0  1823.0     100.00    8.85   \n",
       "\n",
       "                    fscore  nunique  \n",
       "SVM                  92.05   3292.0  \n",
       "ECM                  92.03   3287.0  \n",
       "NaivesBayes          92.03   3287.0  \n",
       "LogisticRegression   92.00   3288.0  \n",
       "WeightedAverage      87.89   3427.0  \n",
       "SimSum               67.67   3939.0  \n",
       "Pseudo Unique ID     62.49   4014.0  \n",
       "Exact Matching       16.26   4823.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_a,df_true_links = run_import_data('dataset_c.csv')\n",
    "df_a = run_preprocessing(df_a)\n",
    "metrics = run_classification(df_a,df_true_links)\n",
    "df_results = pd.DataFrame(metrics,index= ['pairs','matched','TP','FP','FN','presicion','recall','fscore','nunique'])\n",
    "df_results_t = df_results.T.sort_values('fscore',ascending=False)\n",
    "df_results_t.to_csv(\"dataset_c_results.csv\")\n",
    "df_results_t['fscore'].mean()\n",
    "df_results_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
